---
title: "MA679 Final Project"
author: "Lihao Liao, Ruining Jia"
date: "`r format(Sys.time(), '%b %d, %Y')`" 
link-citations: yes
header-includes:
- \usepackage{float}
- \usepackage{mathtools}
- \usepackage{xcolor}
- \usepackage{float}
- \floatplacement{figure}{H}
output: 
  bookdown::pdf_document2
editor_options: 
  chunk_output_type: inline
---

```{r setup, echo = F, message = FALSE}
library(stats)
library(ggplot2)
library(lattice)
library(dplyr)
library(magrittr)
library(Matrix)
library(MASS)
library(reshape)
library(grid)
library(gridExtra)
library(knitr)
#library("formatR")## better format r markdown
library(leaps)
library(randomForest)
library(R.matlab)
library(keras) # for deep learning
#install_keras()
library(tidyverse) # general utility functions
library(caret)
#Sys.setenv(RETICULATE_PYTHON = "/usr/local/bin/python3")
library(reticulate)
#Sys.setenv(TENSORFLOW_PYTHON="/usr/bin/python3")
#devtools::install_github("rstudio/tensorflow")
library(tensorflow)

opts_chunk$set(fig.cap="",
               #fig.width=8, fig.height=6, 
               #fig.path = "./old_sim_res_pics/",
              # fig.pos = "!H",
               out.extra = "",
               dpi=150,
               warning = FALSE)
set.seed(1)
#options(digits=4)

### wrap long codes 
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 50), tidy = TRUE)
```


```{r echo=F, message=F, warning=F}

```


# Introduction 

```{r echo=F, message=F, warning=F, include=F}

########## Z score ###############
files_to_read = list.files(
  path = "./data/Zero_Maze/", ## common path, directory to search within 
  pattern = "binned_zscore.*mat$", # regex pattern, some explanation below
  recursive = TRUE,          # search subdirectories
  full.names = TRUE          # return the full path
)

# read all the matching files
dat.Zscore.full = lapply(files_to_read, function(s){
  temp = readMat(s) ## read files
  return(temp$binned.zscore) #abstract binned behavior data 
  })

############# behavior ##############

files_to_read = list.files(
  path = "./data/Zero_Maze/", ## common path, directory to search within 
  pattern = "binned_behavior.*mat$", # regex pattern, some explanation below
  recursive = TRUE,          # search subdirectories
  full.names = TRUE          # return the full path
)

# read all the matching files
dat.behave.full = lapply(files_to_read, function(s){
  temp = readMat(s) ## read files
  return(temp$binned.behavior) #abstract binned behavior data 
  })  



##### obtain folders names for each sample ####
### name the sample in the data with its corresponding folder name

names = lapply(files_to_read, function(s) gsub(".*Zero_Maze//(.+)/Day_1.*", "\\1", s)) %>% unlist()

names(dat.behave.full) = names
names(dat.Zscore.full) = names


### transpose Z score data so that each row represent each cell ###
dat.Zscore.full = lapply(dat.Zscore.full, function(s) t(s))

######## name each col by the number of time point ##########

dat.Zscore.full = lapply(dat.Zscore.full, function(s) {
  t = dim(s)[2] ## get the number of cols (i.e. time points)
  colnames(s) = 1:t
  return(s)
})

dat.behave.full = lapply(dat.behave.full, function(s) {
  t = dim(s)[2] ## get the number of cols (i.e. time points)
  colnames(s) = 1:t
  rownames(s) = c("close", "open")
  return(s)
})



```



```{r datacleaning, echo=F, message=F, warning=F}

### check and remove the missing data #####
## remove the row of behavior data where both cols ==0

dat.behave.miss = lapply(dat.behave.full, function(s){
  miss = s[,colSums(s)==0]
  colnames(miss)
}) 

dat.behave.clean =  lapply(dat.behave.full, function(s){
  return(s[,colSums(s)!=0])
})

dim.behave.clean = lapply(dat.behave.clean , function(s) {
  dim(s)
})

#### only keep the Z score data where mouse behavior was tracked for each mouse

dat.Zscore.clean = list()

for(i in 1:13){
  dat.Zscore.clean[[i]] = dat.Zscore.full[[i]][,-as.numeric(dat.behave.miss[[i]])]
}

dim.Zscore.clean = lapply(dat.Zscore.clean , function(s) {
  dim(s)
})

############ get the Z score for close arm  ##########
### i.e., row 1 is one in hehavior data 

Zscore.close =list()
for(i in 1:13){
  temp =  dat.behave.clean[[i]][,dat.behave.clean[[i]][1,]==1]
  ids = colnames(temp)
  Zscore.close[[i]] = dat.Zscore.clean[[i]][, ids]
}


############ get the Z score for open arm  ##########
### i.e., row 2 is one in hehavior data 

Zscore.open =list()
for(i in 1:13){
  temp =  dat.behave.clean[[i]][,dat.behave.clean[[i]][2,]==1]
  ids = colnames(temp)
  Zscore.open[[i]] = dat.Zscore.clean[[i]][, ids]
}

```

```{r echo=F, message=F, warning=F, include=F}

dim.behave = lapply(dat.behave.clean , function(s) {
  dim(s)
})

```


```{r Heatmap, echo=F, message=F, warning=F,cache=T}

# for(i in 1:13){
# par(mfrow=c(1,2))
# heatmap(Zscore.close[[i]], main=paste0("Mouse ", i, ": close arm"))
# heatmap(Zscore.open[[i]], main=paste0("Mouse ", i, ": open arm"))
# }


# for(i in 1:2){
# par(mfrow=c(1,2))
# heatmap(Zscore.close[[i]], main=paste0("Mouse ", i, ": close arm"))
# heatmap(Zscore.open[[i]], main=paste0("Mouse ", i, ": open arm"))
# }
```


Here are the plots of neutron counts over time for Mouse 1 and Mouse 2, in which the red line repents the mean neutron counts over time across all the cells. We can clearly see that neutron counts change along time. 
```{r echo=F, message=F, warning=F}
dat.fpca = list() 
train = list()
test = list()
set.seed(123)
for (i in 1:13){
  dat.fpca[[i]] = data.frame(hehavior = t(dat.behave.clean[[i]])[,2], Cell=t(dat.Zscore.clean[[i]]), time = seq(1:dim(dat.behave.clean[[i]])[2]))
  ## only keep the column for open arm for the hehvaior data
  ## each row represents each (temporal) sample
  train.id=sort(sample(nrow(dat.fpca[[i]] ), nrow(dat.fpca[[i]] )*0.7)) ## take 70% of the sample as training data
  train[[i]]= dat.fpca[[i]][train.id,]
  test[[i]]=dat.fpca[[i]][-train.id,]
}
#head(dat.lg[[i]] )


for(i in 1:2){
  measures = dat.fpca[[i]] %>% 
    dplyr::select(contains("Cell")) %>%  
    as.matrix()
  measures.mean = dat.fpca[[i]] %>% 
    dplyr::select(contains("Cell")) %>% 
    summarise(rowMeans(.))
  matplot((measures), type="l", col="grey", xlab="Time", ylab="Nuetron count", 
          main=paste0("Nuetron count over time for Mouse ", i),lty = 1,lwd=1)
  lines(measures.mean, col="red")
}
```



# Baseline model 

If we do not consider the potential time dependence of the neutron activity and mouse behavior and assume that the each (temporal) sample is independent of each other, we can build of logistic regression model to predict and classify the behavior of each mouse (open arm =1, close arm 0). To do so, we need to only keep the column representing open arm in the behavior data since if it is close arm the value in that column would be 0 and if it is open arm the value in that column would be 1. In addition, we need to combine data for Z score (neutron activity) and behavior into one data set, which can be easily done by merge those two data sets by column since we have already confirmed that ith row in each data set both represents the ith (temporal) sample during the EDA. After that, we can build a logistic regression model with mouse behavior (open arm or close arm) as outcome and neutron activity (Z scores) of each cell as predictors. 

We split the data into two data sets: Training and Test for each mouse. Training data were used to build the logistics regression model and test data were used to check the performance of the logtisitc regression model. For each mouse, we first fit the logistic regression model with all the cells (predictors). We then only keep the cells (predictors) that are statistically significant at $5\%$ confidence level and refit the logistic model with only the significant cells. False Negative rate, False Positive rate, and Precision rate were obtained using test data for each mouse and can be found in the following table

```{r echo=F, message=F, warning=F}
#head(t(dat.behave.clean[[1]]))

dat.lg = list() ## data for logistic regression
train = list()
test = list()
set.seed(123)
for (i in 1:13){
  dat.lg[[i]] = data.frame(hehavior = t(dat.behave.clean[[i]])[,2], Cell=t(dat.Zscore.clean[[i]])) 
  ## only keep the column for open arm for the hehvaior data
  ## each row represents each (temporal) sample
  train.id=sort(sample(nrow(dat.lg[[i]] ), nrow(dat.lg[[i]] )*0.7)) ## take 70% of the sample as training data
  train[[i]]= dat.lg[[i]][train.id,]
  test[[i]]=dat.lg[[i]][-train.id,]
}
#head(dat.lg[[i]] )

mod.lg = lapply(train, function(s){
  glm(hehavior ~ ., data = s, family= binomial)
})

## summary the logistic regression for each mouse and the get the significance for each cell
res = lapply(mod.lg, function(s){
  sum = summary(s)
  coef = sum$coefficients %>% as.data.frame()
  colnames(coef)[4] = "P.value"
  return(coef)
})

## summary the logistic regression for each mouse and only keep the cells that are significant
## and then refit the model with only significant cells
mod.significant = lapply(train, function(s){
  mod.full = glm(hehavior ~ ., data = s, family= binomial)
  sum = summary(mod.full)
  coef = sum$coefficients %>% as.data.frame()
  colnames(coef)[4] = "P.value"
  coef.sig = coef %>% filter(P.value < 0.05)
  cell.names = rownames(coef.sig)[-1] ## get the significant cell names (without intercept term)
  formula = as.formula(paste0("hehavior~ ", paste(cell.names , collapse=" + ")))
  mod = glm(formula, data = s, family= binomial)
  return(mod)
})



### get FN, FP, and Precision for test data for each mouse 

mod.precison = lapply(dat.lg, function(s){
  set.seed(123)
  train.id=sort(sample(nrow(s ), nrow(s )*0.7)) ## take 70% of the sample as training data
  train= s[train.id,]
  test=s[-train.id,]
  mod.full = glm(hehavior ~ ., data = train, family= binomial)
  sum = summary(mod.full)
  coef = sum$coefficients %>% as.data.frame()
  colnames(coef)[4] = "P.value"
  coef.sig = coef %>% filter(P.value < 0.05)
  cell.names = rownames(coef.sig)[-1] ## get the significant cell names (without intercept term)
  formula = as.formula(paste0("hehavior~ ", paste(cell.names , collapse=" + ")))
  mod = glm(formula, data = train, family= binomial)
 pred.p = predict(mod, type="response", newdata= test)
  pred = ifelse(pred.p>0.5, "Open", "Close")
  tb = table(True=test$hehavior, Pred = pred)
  FN = tb[2,1]/sum(tb[1,1],tb[2,1])
FP = tb[1,2]/sum(tb[2,2],tb[1,2])
precision = (tb[2,2]+tb[1,1])/sum(tb)
data.frame(FN = FN, FP= FP, Precision = precision)
})


tb.lg = mod.precison %>% 
  unlist() %>% 
  matrix(nrow= 13, byrow=T) %>% 
  as.data.frame()
colnames(tb.lg) = c("False Negative rate", "False Positive rate", "Precision rate")
rownames(tb.lg) = sapply(1:13, function(s) paste("Mouse", s, sep = " "))


kable(tb.lg, caption = "Error rate and precision for logistic regression model of the test data")%>%
  kableExtra::kable_styling(full_width = F,latex_options=c("HOLD_position"))


```

In the following models, we will be using data from Mouse 1. 

# A basic neural network approach (Flatten layer model)

The aim of this work is to predict the future behavior of a mouse given its current neural activity as well previous behaviors and neural activity. We need to decide how far back we would like to look back and how we would like to sample data during the training process for depicting future behavior. For the basic neural network framework, we choose to look back as far as 40 time points ago and draw one data point every 10 time points for predicting the behavior of a mouse for the next time point.


An essential part of the neural network is that we need to process the raw data into a format that a neural network can ingest. To do that, we first need to make sure that the data are numerical. Secondly, we normalized the raw data to make each time series be on the same scale. We then need to obtain a batch of data containing data in the past 40 time points as well as the "current" time point. Given the size of the data we have, if we feed them all in at one time, it might cause a huge computation burden on the computer that can crash R. Therefore, we decided to "chop" the data into small batches and feed the one batch in instead of the whole data. We set the batch size to be 120, which means there are 120 samples in each batch. This is done by a custom generator function based on the mouse data, which outputs a list of two elements: Inputs and targets. For each training epoch, we did 500 steps (batches of samples), which means we "searched" the whole data about 60 times for each training epoch ($120*500*6/6000$).

We first fit a basic neural network model, which is a flattened layer network. A flatten layer will take an array input, as a flatten layer, i.e., an vector. For example, a 2 dimensional $4 \times 4$ array will become a one-dimensional vector of 16 in a flattened layer. The following picture illustrates the flatten layer neural network we have applied in this project. A 2 dimensional $120 \times 111$ array has been fed in and a one-dimensional vector of 13320 has been taken in the first layer. The flatten layer networks will not account for temporal dependence between observations.

![Flatten layer](./refference/Flat.png){width=50%}

We split the data into three parts: training, validation, and test data. Data obtained at the first 3000 time points will be used as training data. Data obtained at the next 1000 time points will be used as validation data. The rest data will be used as test data. 

```{r echo=F, message=F, warning=F}
# generator <- function(data, lookback, delay, min_index, max_index,
#                       shuffle = FALSE, batch_size = 128, step = 6) {
#   if (is.null(max_index))
#     max_index <- nrow(data) - delay - 1
#   i <- min_index + lookback
#   function() {
#     if (shuffle) {
#       rows <- sample(c((min_index+lookback):max_index), size = batch_size)
#     } else {
#       if (i + batch_size >= max_index)
#         i <<- min_index + lookback
#       rows <- c(i:min(i+batch_size-1, max_index))
#       i <<- i + length(rows)
#     }
# 
#     samples <- array(0, dim = c(length(rows),
#                                 lookback / step,
#                                 dim(data)[[-1]]))
#     targets <- array(0, dim = c(length(rows)))
#                       
#     for (j in 1:length(rows)) {
#       indices <- seq(rows[[j]] - lookback, rows[[j]]-1,
#                      length.out = dim(samples)[[2]])
#       samples[j,,] <- data[indices,]
#       targets[[j]] <- data[rows[[j]] + delay,2]
#     }           
#     list(samples, targets)
#   }
# }


generator <- function(data, lookback, delay, min_index, max_index,
                      shuffle = FALSE, batch_size = 128, step = 6) {
  if (is.null(max_index))
    max_index <- nrow(data) - delay - 1
  i <- min_index + lookback
  function() {
    if (shuffle) {
      rows <- sample(c((min_index+lookback):max_index), size = batch_size)
    } else {
      if (i + batch_size >= max_index)
        i <<- min_index + lookback
      rows <- c(i:min(i+batch_size-1, max_index))
      i <<- i + length(rows)
    }

    samples <- array(0, dim = c(length(rows),
                                lookback / step,
                                dim(data)[[-1]]))
    targets <- array(0, dim = c(length(rows)))
                      
    for (j in 1:length(rows)) {
      indices <- seq(rows[[j]] - lookback, rows[[j]]-1,
                     length.out = dim(samples)[[2]])
      samples[j,,] <- data[indices,]
      targets[[j]] <- data[rows[[j]] + delay,1] ## behavior at the first col
    }           
    list(samples, targets)
  }
}

```

```{r echo=F, message=F, warning=F, include=F}
data <- data.matrix(dat.lg[[1]])

mean <- apply(data, 2, mean)
std <- apply(data, 2, sd)
data <- scale(data, center = mean, scale = std)

normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

max <- apply(data,2,max)
min <- apply(data,2,min)

data <- apply(data, 2, normalize)

#dim(data)
#head(data)
```

```{r echo=F, message=F, warning=F, include =F}
lookback = 40 ## observations will go back 40 time points
step = 6 ## observations will be sampled every 6 time points
delay = 1 ## prediction will be 1 timepoint in the future
batch_size = 120 ## the number of sample per batch

train_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = 4000,
  shuffle = F,
  step = step, 
  batch_size = batch_size
)

val_gen = generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 4001,
  max_index = 5000,
  step = step,
  batch_size = batch_size
)

test_gen <- generator(
  data,
  lookback = lookback,
  delay = 0,
  min_index = 5001,
  max_index = NULL,
  step = step,
  batch_size = batch_size
)

# How many steps to draw from val_gen in order to see the entire validation set
val_steps <- (5000 - 4001 - lookback) / batch_size

# How many steps to draw from test_gen in order to see the entire test set
test_steps <- (nrow(data) - 5001 - lookback) / batch_size
```



```{r echo=F, message=F, warning=F, include =F}
model <- keras_model_sequential() %>%
  layer_flatten(input_shape = c(lookback/step, dim(data)[-1])) %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")


model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

history.flat <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 500,
  epochs = 10,
  workers = 4,
  validation_data = val_gen,
  validation_steps = val_steps
)

# model <- keras_model_sequential() %>%
#   layer_flatten(input_shape = c(lookback / step, dim(data)[-1])) %>%
#   layer_dense(units = 128, activation = "relu") %>%
#   layer_dense(units = 64, activation = "relu") %>%
#   layer_dense(units = 1)
# 
# summary(model)

# model %>% compile(optimizer = optimizer_rmsprop(),
#                   loss = "binary_crossentropy")
# history <- model %>% fit(
#   train_gen_data[[1]],train_gen_data[[2]],
#   batch_size = 120,
#   epochs = 20,
#   use_multiprocessing = T
# )
```


```{r echo=F, message=F, warning=F, out.width="60%"}
plot(history.flat, main="Flat RNN")

#model %>% evaluate(test_gen,verbose = 0)
```

```{r echo=F, message=F, warning=F, eval=F}
batch_size_plot <- 1200
lookback_plot <- 40
step_plot <- 1

pred_gen <- generator(
  data,
  lookback = lookback_plot,
  delay = 0,
  min_index = 4001,
  max_index = NULL,
  shuffle = FALSE,
  step = step_plot,
  batch_size = batch_size_plot
)
pred_gen_data <- pred_gen()

inputdata <- pred_gen_data[[1]][, , 2]

pred_out <- model %>%
  predict(inputdata)
```

# Recurrent neural network (RNN)
A recurrent neural network (RNN) has often been used to predict an event that does not only depends on the predictors of the current time point but also depends on the previous events. In this project, the behavior of the mouse does not only depend on its current neutral activity but can also be affected by its previous behaviors. From the plot of the behavior of Mouse 1 during the first 200 time points, we can see that its behavior in the next time point tended to be consistent with its previous time points. 

```{r echo=F, warning = F,message=F, out.width="60%"}
plot(dat.lg[[1]]$hehavior[1:200], ylab = "behvaior", Main="Behvavior at the first 200 time points")
```

The following plot shows the basic idea for an one-layer many-to-one RNN. 

![RNN](./refference/RNN.png){width=50%}

Here is the unfolded-version of a simple RNN 

![Unfolded RNN](./refference/unfolded RNN.png){width=50%}
The general steps included in a simple RNN are: 

1. Input $X(t_1)$ is fed into RNN and generate an output $h(t_1)$
2. Output $h(t_1)$ and  $X(t_2)$ will both serve as input for the second step, which generate output $h(t_2)$.
3. Output $h(t_1)$ and  $X(t_3)$ will both serve as input for the second step, which generate output $h(t_3)$.
4. This goes on until hit the final output $h(t_{k+1})$, where k is the number of time points we would like to look back.


## Simple RNN

We first fit a simple RNN that look back as far as 40 time points ago to predict the behavior at the next time point. The accuracy of validation is about 0.9, which is not bad can be better. 

```{r echo=F, message=F, warning=F, include =F}
model <- keras_model_sequential() %>% 
  layer_simple_rnn(units = 32, input_shape = list(NULL, dim(data)[[-1]])) %>% 
  layer_dense(units = 2, activation = "softmax") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "binary_crossentropy",
  # loss = "binary_crossentropy",
  metrics = c("accuracy")
)

history.rnn <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 500,
  epochs = 10,
  workers  = 4,
  validation_data = val_gen,
  validation_steps = val_steps
  
)
```

```{r echo=F, message=F, warning=F, out.width="60%"}
plot(history.rnn, main="simple RNN")

#model %>% evaluate(test_gen()[[1]][,,1],test_gen()[[-1]], verbose = 0)
```

## Long Short Term Memory RNN (LSTM)

Simple RNN is a good baseline RNN. The major advantage of RNN is that we believe or assume that it is able to provide previous information in the current training, i.e., it can take temperate dependency into consideration during model training. However, long-term dependencies are often missed during the optimization process. For example, a simple RNN might perform not as well as we expect it to be, if the prediction of current or future behavior of a mouse is related not only to its recent status but also its status in the very past. 

Long Short Term Memory RNN model (LSTM) will remember the previous information (long-term dependency) and use it for processing current input, as it is illustrated in the following picture. LSTM will "carry out" the previous output and feed it into the current training process. In other words, LSTM will not only consider the temporal dependency between observations but it will also consider the temporal dependency between predictions. 

The basic idea of LSTM RNN is illustrated in the following diagram. Within each recurrent layer, there are four more interacting layers that controls the information fed in the cell state. 

![LSTM](./refference/LSTM.png){width=50%}
This following plot shows a more detailed framework of LSTM. 

![More detailed LSTM](./refference/LSTM_detail.png){width=50%}

1. The Forget gate layer decides which information will be tossed away from the cell state
2. The Input gate layer decides decides which new information will be fed into the cell state. 
3. This information has then been passed into a tanh layer to create a vector of new candidate information, which will be fed into the new cell state
4. The output gate decides which information will be output 


The accuracy of validation for the LSTM RNN is about 0.75, which is quite poor. However, this might be caused by overfitting issues, which will be addressed later

```{r echo=F, message=F, warning=F, include =F}
model <- keras_model_sequential() %>% 
  layer_lstm(units = 32, input_shape = list(NULL, dim(data)[[-1]])) %>% 
  layer_dense(units = 2, activation = "softmax") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "binary_crossentropy",
  # loss = "binary_crossentropy",
  metrics = c("accuracy")
)

history.lstm <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 500,
  epochs = 10,
  workers  = 4,
  validation_data = val_gen,
  validation_steps = val_steps
  
)
```



```{r echo=F, message=F, warning=F, out.width="60%"}
plot(history.lstm, main="LSTM RNN")

#model %>% evaluate(test_gen()[[1]][,,1],test_gen()[[-1]], verbose = 0)
```


```{r, echo=F, eval=F}
data <- data.matrix(dat.lg[[1]])


# Preparing a subset for training and other for testing
index <- createDataPartition(data[,1], p=0.7, list=FALSE)
df.training <- data[index,]
df.test <- data[-index,]

# Size and format of data frame
X_train <- df.training[,-1] %>% 
 scale()
y_train <- (df.training[,1])

X_test <- df.test[,-1] %>% 
 scale()
y_test <- (df.test[,1])


# Network design
 model <- keras_model_sequential()
 model %>%
# Input layer
 layer_dense(units = 256, activation = "relu", input_shape =  ncol(X_train)) %>% 
 layer_dropout(rate = 0.4) %>% 
# Hidden layer
 layer_dense(units = 75, activation = "relu") %>%
# Output layer
 layer_dropout(rate = 0.3) %>%
 layer_dense(units = 2, activation = "sigmoid")

  model %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  # loss = "binary_crossentropy",
  metrics = c("accuracy")
)

# Running our data
model %>% fit(
 X_train, y_train, 
 epochs = 100, 
 batch_size = 5,
 validation_split = 0.3
)

summary(model)
 
```



## Gated Recurrent Unit RNN (GRU)

Gated recurrent unit is very similar to LSTM as they share the same principle. However GRU usually requires fewer parameters than LSTM, which makes it easier to run but also makes it have less representational power than LSTM. 

The accuracy of validation for the GRU RNN is about the same as LSTM, which is quite poor. However, this might be caused by overfitting issues, which will be addressed later


```{r echo=F, message=F, warning=F, include =F}
model <- keras_model_sequential() %>% 
  layer_gru(units = 32, input_shape = list(NULL, dim(data)[[-1]]),
            activation = "relu") %>% 
  layer_dense(units = 2, activation = "softmax") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "binary_crossentropy",
  # loss = "binary_crossentropy",
   metrics = c("accuracy")
)

history.gru <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 500,
  epochs = 10,
  workers = 4,
  validation_data = val_gen,
  validation_steps = val_steps
)
```

```{r echo=F, message=F, warning=F, out.width="60%"}
plot(history.gru, mian="GRU RNN")

#model %>% evaluate(x_test, y_test,verbose = 0)
```


## Address overfitting issues

It is clear that there is overfitting issues for simple, LSTM, and GRU RNN models as the training and validation curves start to diverge from each other after a few training epochs. For the simple RNN model, we can randomly drop out a certain percentage of connections between layers to prevent overfitting issues. For LSTM and GRU RNN, such dropout should be happen at each time step. In addition, a temporally constant dropout was applied to the inner recurrent activation, which will help to regularize he representation of the recurrent gates. In this project, we decided to drop out $20\%$ of the connections between the input and first hidden layer and  drop out $20\%$ of the inner recurrent activation for the LSTM and GRU RNN. 

### LSTM

```{r echo=F, message=F, warning=F, include =F}

model <- keras_model_sequential() %>% 
  layer_lstm(units = 32, dropout = 0.2, recurrent_dropout = 0.2,
            input_shape = list(NULL, dim(data)[[-1]])) %>% 
  layer_dense(units = 2, activation = "softmax") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model.lsmt =model%>% compile(
  optimizer = optimizer_rmsprop(),
  metrics = c("accuracy"),
  loss = "binary_crossentropy"
)

history.lstm.overfit <- model.lsmt %>% fit_generator(
  train_gen,
  steps_per_epoch = 500,
  epochs = 10,
  workers = 4,
  validation_data = val_gen,
  validation_steps = val_steps
)
```



```{r echo=F, message=F, warning=F, out.width="60%"}
plot(history.lstm.overfit, main="LSTM RNN no overfitting")

#model %>% evaluate(test_gen(),verbose = 0)
```

```{r echo=F, message=F, warning=F}
pred_gen_data <- test_gen()
inputdata <- pred_gen_data[[1]]
true =  pred_gen_data[[2]]
pred.p <- predict(model.lsmt,inputdata)
  pred = ifelse(pred.p>0.5, 1, 0)
  tb = table(True=true, Pred = pred)
  if(sum(dim(tb)) <4){
    precision =(tb[1,1])/sum(tb)
  }else{
precision = (tb[2,2]+tb[1,1])/sum(tb)}
res=data.frame(Precision = precision)

```

### GRU

```{r echo=F, message=F, warning=F, include =F}

model <- keras_model_sequential() %>% 
  layer_gru(units = 32, dropout = 0.2, recurrent_dropout = 0.2,
            input_shape = list(NULL, dim(data)[[-1]])) %>% 
  layer_dense(units = 2, activation = "softmax") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model.gru =model %>% compile(
  optimizer = optimizer_rmsprop(),
  metrics = c("accuracy"),
  loss = "binary_crossentropy"
)

history.gru.overfit <- model.gru %>% fit_generator(
  train_gen,
  steps_per_epoch = 500,
  epochs = 10,
  workers = 4,
  validation_data = val_gen,
  validation_steps = val_steps
)


```



```{r echo=F, message=F, warning=F, out.width="60%"}
plot(history.gru.overfit, main="GRU RNN no overfitting")

```

```{r echo=F, message=F, warning=F}
pred_gen_data <- test_gen()
inputdata <- pred_gen_data[[1]]
true =  pred_gen_data[[2]]
pred.p <- predict(model.gru,inputdata)
  pred = ifelse(pred.p>0.5, 1, 0)
  tb = table(True=true, Pred = pred)
  if(sum(dim(tb)) <4){
    precision =(tb[1,1])/sum(tb)
  }else{
precision = (tb[2,2]+tb[1,1])/sum(tb)}
res=data.frame(Precision = precision)

```


The training and validation curves did not diverge from each other for the first training epochs for both LSTM and GRU RNN, which suggested that we have successfully preventing the overfitting issues. In addition, the accuracy of validation for the GRU RNN is about the same as LSTM, which is close to 1. The accuracy test data for the LSTM is about $83.33\%$ and is about 1 for the GRU RNN. 

# models for all mice

We decided to perform both LSTM and GRU RNN on all the mouse since GRU RNN requires less computation power and showed very similar or even better results than LSTM RNN. 

```{r echo=F, message=F, warning=F, include=F}
data_clean = function(data){
mean <- apply(data, 2, mean)
std <- apply(data, 2, sd)
data <- scale(data, center = mean, scale = std)

normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

max <- apply(data,2,max)
min <- apply(data,2,min)

data <- apply(data, 2, normalize)
return(data)
}

history.lstm = list()
precision.lstm = c()
history.gru = list()
precision.gru = c()

for (i in 1:13){
  data <- data.matrix(dat.lg[[i]])
  data = data_clean(data)

lookback = 40 ## observations will go back 40 time points
step = 6 ## observations will be sampled every 6 time points
delay = 1 ## prediction will be 1 timepoint in the future
batch_size = 120 ## the number of sample per batch
max_line = nrow(data)
train.max = floor(max_line*0.7)
val.min = train.max+1
val.max = floor(max_line*0.9)
test.min = val.max +1

train_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = train.max,
  shuffle = F,
  step = step, 
  batch_size = batch_size
)

val_gen = generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = val.min,
  max_index = val.max,
  step = step,
  batch_size = batch_size
)

test_gen <- generator(
  data,
  lookback = lookback,
  delay = 0,
  min_index = test.min,
  max_index = NULL,
  step = step,
  batch_size = batch_size
)

# How many steps to draw from val_gen in order to see the entire validation set
val_steps <- (val.max - val.min - lookback) / batch_size

# How many steps to draw from test_gen in order to see the entire test set
test_steps <- (nrow(data) - test.min - lookback) / batch_size
model <- keras_model_sequential() %>% 
  layer_gru(units = 32, dropout = 0.2, recurrent_dropout = 0.2,
            input_shape = list(NULL, dim(data)[[-1]])) %>% 
  layer_dense(units = 2, activation = "softmax") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model.gru<-model %>% compile(
  optimizer = optimizer_rmsprop(),
  metrics = c("accuracy"),
  loss = "binary_crossentropy"
)

# history.gru[[i]] <- model %>% fit_generator(
#   train_gen,
#   steps_per_epoch = 500,
#   epochs = 10,
#   workers = 4,
#   validation_data = val_gen,
#   validation_steps = val_steps
# )

#plot(history.gru.overfit, main="GRU RNN no overfitting")
#print(p)
### accuracy ####
pred_gen_data <- test_gen()
inputdata <- pred_gen_data[[1]]
true =  pred_gen_data[[2]]
pred.p <- predict(model.gru,inputdata)
  pred = ifelse(pred.p>0.5, 1, 0)
  tb = table(True=true, Pred = pred)
  if(sum(dim(tb)) <4){
    precision.gru[i] =(tb[1,1])/sum(tb)
  }else{
precision.gru[i] = (tb[2,2]+tb[1,1])/sum(tb)}
  
model <- keras_model_sequential() %>% 
  layer_lstm(units = 32, input_shape = list(NULL, dim(data)[[-1]])) %>% 
  layer_dense(units = 2, activation = "softmax") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model.lstm = model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "binary_crossentropy",
  # loss = "binary_crossentropy",
  metrics = c("accuracy")
)

# history.lstm[[i]] <- model %>% fit_generator(
#   train_gen,
#   steps_per_epoch = 500,
#   epochs = 10,
#   workers  = 4,
#   validation_data = val_gen,
#   validation_steps = val_steps
# )  

### accuracy ####
pred_gen_data <- test_gen()
inputdata <- pred_gen_data[[1]]
true =  pred_gen_data[[2]]
pred.p <- predict(model.lstm,inputdata)
  pred = ifelse(pred.p>0.5, 1, 0)
  tb = table(True=true, Pred = pred)
  if(sum(dim(tb)) <4){
    precision.lstm[i] =(tb[1,1])/sum(tb)
  }else{
precision.lstm[i] = (tb[2,2]+tb[1,1])/sum(tb)}
  
print(i)
}


```

## LSTM RNN

```{r echo=F, message=F, warning=F, out.width="60%", eval=F}
for(i in 1:13){
p= plot(history.lstm[[i]], main="GRU RNN no overfitting")
print(p)
}
```

## GRU RNN
```{r echo=F, message=F, warning=F, out.width="60%", eval=F}
for(i in 1:13){
p= plot(history.gru[[i]], main="GRU RNN no overfitting")
print(p)
}
```


## Compare to baseline model 

```{r echo=F, message=F, warning=F, out.width="60%"}
res= data.frame(LSTM = precision.lstm, GRU = precision.gru, Logistic=tb.lg$`Precision rate`)

kable(res, caption = "Accuracy of LSTM RNN, GRU RNN, and logistic regression model of the test data")%>%
  kableExtra::kable_styling(full_width = F,latex_options=c("HOLD_position"))


```


# Take Away Message

1. RNN is generally performs better than simple flatten-layer NN and logistic regression since it is able to address temporal dependency between observations
2. LSTM and GRU RNN performs better than simple RNN since it is able to address long-term dependency, which is evidently required for predicting future behavior of a mouse in this work. 
3. The performance of LSTM and GRU RNN can be affected by overfitting issues, which is quite normal when we have high-dimensional data. 
4. Overfitting issues can generally be prevented by dropout a certain percentage of connections between layers. For LSTM and GRU RNN,  a temporally constant dropout was applied to the inner recurrent activation would also help to prevent  overfitting. 
5. When the performance of GRU and LSTM RNN are very similar, we may choose to use GRU RNN since it requires less computation power. 
6. Approporiate RNN models should be selected based on the data and aim of the study. 


